{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 10\n",
    "# Jhamil Crespo Rejas\n",
    "# Ingenieria en Ciencias de la Computacion \n",
    "\n",
    "## Comparación de Imágenes con **DeepFace**\n",
    "\n",
    "Este código utiliza la biblioteca **DeepFace** para realizar una tarea de comparación de imágenes faciales, determinando si dos imágenes corresponden a la misma persona. Se basa en el modelo de reconocimiento **ArcFace** y el backend de detección facial **RetinaFace**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Importación de la Biblioteca**\n",
    "```python\n",
    "from deepface import DeepFace\n",
    "```\n",
    "- Se importa el módulo `DeepFace`, que proporciona una API fácil de usar para tareas de reconocimiento y análisis facial.\n",
    "- **DeepFace** soporta múltiples modelos de reconocimiento y backends de detección, lo que permite seleccionar la combinación más adecuada para diferentes casos de uso.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Definición de las Rutas de las Imágenes**\n",
    "```python\n",
    "img1 = r\"C:\\Users\\Jhamil\\Desktop\\IA2\\Lab10\\img_align_celeba\\img_align_celeba\\000023.jpg\"\n",
    "img2 = r\"C:\\Users\\Jhamil\\Desktop\\IA2\\Lab10\\img_align_celeba\\img_align_celeba\\000015.jpg\"\n",
    "```\n",
    "- **`img1`** y **`img2`** son las rutas de las imágenes que se van a comparar.\n",
    "- Estas rutas deben apuntar a imágenes donde los rostros sean visibles para que el modelo pueda procesarlas correctamente.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Comparación de Imágenes con `DeepFace.verify`**\n",
    "```python\n",
    "resultado = DeepFace.verify(\n",
    "    img1_path=img1,\n",
    "    img2_path=img2,\n",
    "    model_name=\"ArcFace\",\n",
    "    detector_backend=\"retinaface\"\n",
    ")\n",
    "```\n",
    "- **`DeepFace.verify`**:\n",
    "  - Esta función compara las dos imágenes proporcionadas y devuelve un diccionario con los resultados.\n",
    "  - Argumentos:\n",
    "    - **`img1_path`** y **`img2_path`**: Rutas de las imágenes que se compararán.\n",
    "    - **`model_name=\"ArcFace\"`**: Selecciona el modelo de reconocimiento facial **ArcFace**, conocido por su precisión gracias a su función de pérdida angular.\n",
    "    - **`detector_backend=\"retinaface\"`**: Utiliza **RetinaFace** como backend de detección facial, asegurando detección y alineación precisa de los rostros antes de procesarlos.\n",
    "- **Proceso interno**:\n",
    "  1. **Detección Facial**:\n",
    "     - **RetinaFace** detecta los rostros en ambas imágenes y los alinea utilizando landmarks faciales.\n",
    "  2. **Generación de Representaciones Embebidas**:\n",
    "     - **ArcFace** convierte cada rostro detectado en un vector embebido de 512 dimensiones, que representa las características únicas del rostro.\n",
    "  3. **Cálculo de Similitud**:\n",
    "     - La similitud entre las dos representaciones se calcula utilizando una métrica de distancia (por defecto, distancia de coseno).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Impresión de Resultados**\n",
    "```python\n",
    "print(\"¿Son la misma persona?:\", resultado[\"verified\"])\n",
    "print(\"Distancia de similitud:\", resultado[\"distance\"])\n",
    "```\n",
    "- **`resultado[\"verified\"]`**:\n",
    "  - Retorna un valor booleano (`True` o `False`) que indica si las imágenes corresponden a la misma persona.\n",
    "- **`resultado[\"distance\"]`**:\n",
    "  - Muestra la distancia calculada entre las dos representaciones embebidas.\n",
    "  - Una distancia menor indica mayor similitud, mientras que una mayor distancia sugiere que las imágenes no corresponden a la misma persona.\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Son la misma persona?: False\n",
      "Distancia de similitud: 0.7341791368557851\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Comparar imágenes con ArcFace y RetinaFace\n",
    "img1 = r\"C:\\Users\\Jhamil\\Desktop\\IA2\\Lab10\\img_align_celeba\\img_align_celeba\\000023.jpg\"\n",
    "img2 = r\"C:\\Users\\Jhamil\\Desktop\\IA2\\Lab10\\img_align_celeba\\img_align_celeba\\000015.jpg\"\n",
    "\n",
    "resultado = DeepFace.verify(\n",
    "    img1_path=img1,\n",
    "    img2_path=img2,\n",
    "    model_name=\"ArcFace\",\n",
    "    detector_backend=\"retinaface\"\n",
    ")\n",
    "\n",
    "print(\"¿Son la misma persona?:\", resultado[\"verified\"])\n",
    "print(\"Distancia de similitud:\", resultado[\"distance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Atributos Faciales con **DeepFace**\n",
    "\n",
    "Este código utiliza la función `DeepFace.analyze` para extraer información avanzada sobre un rostro en una imagen, incluyendo estimaciones de edad, género, emoción predominante y raza. Es una herramienta poderosa para análisis demográfico y comportamental.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Función Principal: `DeepFace.analyze`**\n",
    "```python\n",
    "analisis = DeepFace.analyze(\n",
    "    img_path=img1,\n",
    "    actions=[\"age\", \"gender\", \"emotion\", \"race\"]\n",
    ")\n",
    "```\n",
    "- **`DeepFace.analyze`**:\n",
    "  - Esta función procesa la imagen proporcionada y devuelve un análisis detallado de los atributos faciales.\n",
    "  - Argumentos:\n",
    "    - **`img_path=img1`**: Especifica la ruta de la imagen que será analizada.\n",
    "    - **`actions=[\"age\", \"gender\", \"emotion\", \"race\"]`**: Lista de atributos faciales que se desea analizar. Se pueden solicitar uno o varios de los siguientes:\n",
    "      - **`age`**: Estimación de la edad.\n",
    "      - **`gender`**: Predicción del género (masculino o femenino).\n",
    "      - **`emotion`**: Emoción predominante en el rostro (feliz, triste, neutral, etc.).\n",
    "      - **`race`**: Predicción de la raza predominante en el rostro.\n",
    "\n",
    "#### **Proceso Interno de `DeepFace.analyze`**:\n",
    "1. **Detección Facial**:\n",
    "   - El modelo detecta el rostro en la imagen proporcionada y lo alinea automáticamente.\n",
    "2. **Extracción de Características**:\n",
    "   - Se generan características faciales utilizando redes neuronales preentrenadas, adaptadas para cada acción solicitada.\n",
    "3. **Predicciones**:\n",
    "   - El modelo predice los valores de cada atributo solicitado, basándose en las características extraídas.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Impresión de los Resultados**\n",
    "```python\n",
    "print(\"Edad estimada:\", analisis[0][\"age\"])\n",
    "print(\"Género:\", analisis[0][\"gender\"])\n",
    "print(\"Emoción dominante:\", analisis[0][\"emotion\"])\n",
    "print(\"Raza predominante:\", analisis[0][\"dominant_race\"])\n",
    "```\n",
    "- El resultado de `DeepFace.analyze` es una lista de diccionarios donde cada diccionario corresponde a un rostro detectado en la imagen. En este caso, estamos procesando el primer rostro detectado (`analisis[0]`).\n",
    "- Los atributos extraídos incluyen:\n",
    "  - **`age`**: Predice la edad aproximada del individuo.\n",
    "  - **`gender`**: Devuelve el género más probable (masculino o femenino).\n",
    "  - **`emotion`**: Identifica la emoción predominante en el rostro (ejemplo: feliz, neutral, triste).\n",
    "  - **`dominant_race`**: Predice la raza predominante entre varias categorías predefinidas.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejemplo de Salida**\n",
    "Si `img1` contiene un rostro, los resultados podrían ser los siguientes:\n",
    "```\n",
    "Edad estimada: 28\n",
    "Género: Masculino\n",
    "Emoción dominante: Feliz\n",
    "Raza predominante: Caucásica\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edad estimada: 25\n",
      "Género: {'Woman': np.float32(0.03026013), 'Man': np.float32(99.96974)}\n",
      "Emoción dominante: {'angry': np.float32(0.8079119), 'disgust': np.float32(0.0048881094), 'fear': np.float32(2.1780863), 'happy': np.float32(0.65824217), 'sad': np.float32(5.308478), 'surprise': np.float32(0.0025629418), 'neutral': np.float32(91.03983)}\n",
      "Raza predominante: white\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Analizar atributos faciales (edad, género, emoción, raza)\n",
    "analisis = DeepFace.analyze(\n",
    "    img_path=img1,\n",
    "    actions=[\"age\", \"gender\", \"emotion\", \"race\"]\n",
    ")\n",
    "\n",
    "print(\"Edad estimada:\", analisis[0][\"age\"])\n",
    "print(\"Género:\", analisis[0][\"gender\"])\n",
    "print(\"Emoción dominante:\", analisis[0][\"emotion\"])\n",
    "print(\"Raza predominante:\", analisis[0][\"dominant_race\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
